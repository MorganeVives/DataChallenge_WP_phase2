{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,LSTM, RepeatVector, TimeDistributed\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from vmdpy import VMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "#import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.helper_functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wp1 = pd.read_csv('Data/Preprocessing/WP1_train_preprocessed.csv', sep=',')\n",
    "train_wp2 = pd.read_csv('Data/Preprocessing/WP2_train_preprocessed.csv', sep=',')\n",
    "train_wp3 = pd.read_csv('Data/Preprocessing/WP3_train_preprocessed.csv', sep=',')\n",
    "train_wp4 = pd.read_csv('Data/Preprocessing/WP4_train_preprocessed.csv', sep=',')\n",
    "train_wp5 = pd.read_csv('Data/Preprocessing/WP5_train_preprocessed.csv', sep=',')\n",
    "train_wp6 = pd.read_csv('Data/Preprocessing/WP6_train_preprocessed.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wp1 = pd.read_csv('Data/Preprocessing/WP1_test_preprocessed.csv', sep=',')\n",
    "test_wp2 = pd.read_csv('Data/Preprocessing/WP2_test_preprocessed.csv', sep=',')\n",
    "test_wp3 = pd.read_csv('Data/Preprocessing/WP3_test_preprocessed.csv', sep=',')\n",
    "test_wp4 = pd.read_csv('Data/Preprocessing/WP4_test_preprocessed.csv', sep=',')\n",
    "test_wp5 = pd.read_csv('Data/Preprocessing/WP5_test_preprocessed.csv', sep=',')\n",
    "test_wp6 = pd.read_csv('Data/Preprocessing/WP6_test_preprocessed.csv', sep=',')\n",
    "test_dates = pd.read_csv('Data/Initial/test.csv', sep=',').date.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['date','wd','forecast_time', 'forecast', \"forecast_dist\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture du modèle:\n",
    "def build_model(data,n_neurons,n_steps_out):\n",
    "    model = Sequential()\n",
    "    n_neurons_2= int(n_neurons/2)\n",
    "    model.add(LSTM(n_neurons,activation='relu',return_sequences=True,input_shape=(data.shape[1],data.shape[2])))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(n_neurons,activation='relu',return_sequences=False))\n",
    "    model.add(Dropout(0.1))\n",
    "    #model.add(Dense(n_neurons/4,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(n_steps_out,activation='sigmoid'))\n",
    "\n",
    "  \n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture du modèle:\n",
    "def build_model_seq2seq(data):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(data.shape[1],data.shape[2])))\n",
    "    model.add(RepeatVector(n_steps_out))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_cross_validation(X, y, epochs, n_neurons):\n",
    "    model = build_model(X,n_neurons)\n",
    "    print(model.summary())\n",
    "    model.save_weights('model.h5')\n",
    "    #scaler_X = MinMaxScaler(feature_range=(0,1))\n",
    "    #X = scaler_X.fit_transform(X)\n",
    "\n",
    "    print('-----------DNN CROSS VALIDATION BEGINNING-----------')\n",
    "    split = 10\n",
    "    kf = KFold(n_splits=split, shuffle=True)       \n",
    "    dnn_rmse_scores = []\n",
    "    dnn_mae_scores = []\n",
    "    i = 1\n",
    "    for (train_index, test_index) in kf.split(pd.DataFrame(X1), pd.DataFrame(y)):\n",
    "        X_train, X_test = pd.DataFrame(X).iloc[train_index], pd.DataFrame(X).iloc[test_index]\n",
    "        Y_train, Y_test = pd.DataFrame(y).iloc[train_index],pd.DataFrame(y).iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, Y_train,epochs = epochs, verbose=2,batch_size=4096)\n",
    "\n",
    "        prediction = model.predict(X_test)\n",
    "        dnn_rmse_scores.append(mean_squared_error(Y_test, prediction,squared=False))\n",
    "        dnn_mae_scores.append(mean_absolute_error(Y_test, prediction))\n",
    "        model.load_weights('model.h5')\n",
    "        print(show_evaluation(prediction, Y_test))\n",
    "        print(f'-------------------FOLD {i}-----------------')\n",
    "        i+=1\n",
    "\n",
    "    print('---------------CROSS VALIDATION COMPLETE-------------')\n",
    "    print('--------------------------RMSE-----------------------')\n",
    "    display_scores(dnn_rmse_scores)\n",
    "    print('--------------------------MAE------------------------')\n",
    "    display_scores(dnn_mae_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vmd(y,k):\n",
    "    \n",
    "    #Intrinsic mode generation\n",
    "     #Empirical Mode Decomposition\n",
    "    #. some sample parameters for VMD  \n",
    "    alpha = 1       # moderate bandwidth constraint  \n",
    "    tau = 0.           # noise-tolerance (no strict fidelity enforcement)  \n",
    "    K = k              # k modes  \n",
    "    DC = 0             # no DC part imposed  \n",
    "    init = 1           # initialize omegas uniformly  \n",
    "    tol = 1e-7\n",
    "    u, u_hat, omega = VMD(y,alpha, tau, K, DC, init, tol)\n",
    "    df_vmfs = pd.DataFrame()\n",
    "    #Integration in the dataframe\n",
    "    for num, imf in enumerate(u):\n",
    "        #print('----Creating VMFwp{0} EMD columns----'.format(num+1))\n",
    "        df_vmfs['IMFwp{0}'.format(num+1)] = imf\n",
    "    return df_vmfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp1_X = train_wp1[[c for c in train_wp1 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "#train_size = int(len(wp1_X) * 0.67)\n",
    "#test_size = len(wp1_X) - train_size\n",
    "#train, test = wp1_X[:35100], wp1_X[35100:]\n",
    "\n",
    "X1 = wp1_X.drop('wp', axis=1),\n",
    "y1 = wp1_X['wp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wp1_X = train_wp1[[c for c in train_wp1 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wp1_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences.iloc[i:end_ix, :-1], sequences.iloc[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences_seq2seq(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out-1\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences.iloc[i:end_ix, :-1], sequences.iloc[end_ix-1:out_end_ix, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences_seq2seq_yincluded(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out-1\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences.iloc[i:end_ix,], sequences.iloc[end_ix-1:out_end_ix, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences_vmf(sequences_in,sequences_out, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences_in)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out-1\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences_in):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences_in.iloc[i:end_ix,:-1], sequences_in.iloc[end_ix-1:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_steps = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = split_sequences(wp1_X, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmf_1=vmd(y1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in = 36\n",
    "n_steps_out = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vmf,y_vmf = split_sequences_vmf(wp1_X,vmf_1['IMFwp1'],n_steps_in,n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_sequences_seq2seq(wp1_X, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y, y_y = split_sequences_seq2seq_yincluded(wp1_X, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X,200,n_steps_out)\n",
    "#model.fit(X,y,epochs=1,verbose=2)\n",
    "#print(type((model.predict(X1).reshape(len(X1),))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1636/1636 - 335s - loss: 0.3603 - root_mean_squared_error: 0.6003\n",
      "Epoch 2/5\n",
      "1636/1636 - 264s - loss: 0.3653 - root_mean_squared_error: 0.6044\n",
      "Epoch 3/5\n",
      "1636/1636 - 269s - loss: 0.3756 - root_mean_squared_error: 0.6128\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GERALD~1.CAS\\AppData\\Local\\Temp/ipykernel_7248/3686460438.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\gerald.casterou\\venv\\datachallenge_wp_phase2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gerald.casterou\\venv\\datachallenge_wp_phase2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gerald.casterou\\venv\\datachallenge_wp_phase2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gerald.casterou\\venv\\datachallenge_wp_phase2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gerald.casterou\\venv\\datachallenge_wp_phase2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\gerald.casterou\\venv\\datachallenge_wp_phase2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gerald.casterou\\venv\\datachallenge_wp_phase2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X,y,epochs=5,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_vmf,y_vmf,epochs=5,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_y,y_y,epochs=5,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = split_sequences_seq2seq(wp1_X, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = build_model_seq2seq(X)\n",
    "#model.fit(X,y,epochs=20,verbose=2,batch_size=4096)\n",
    "#print(type((model.predict(X1).reshape(len(X1),))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp2_X = train_wp2[[c for c in train_wp2 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X2 = wp2_X.drop('wp', axis=1)\n",
    "y2 = wp2_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_cross_validation(X2, y2, 20, 574)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp3_X = train_wp3[[c for c in train_wp3 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X3 = wp3_X.drop('wp', axis = 1)\n",
    "y3 = wp3_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_cross_validation(X3, y3, 20, 574)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp4_X = train_wp4[[c for c in train_wp4 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X4 = wp4_X.drop('wp', axis = 1)\n",
    "y4 = wp4_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_cross_validation(X4, y4, 20, 574)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp5_X = train_wp5[[c for c in train_wp5 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X5 = wp5_X.drop('wp', axis = 1)\n",
    "y5 = wp5_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_cross_validation(X5, y5, 20, 574)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp6_X = train_wp6[[c for c in train_wp6 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X6 = wp6_X.drop('wp', axis = 1)\n",
    "y6 = wp6_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_cross_validation(X6, y6, 20, 574)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_test = ['date','wd','forecast_time', 'forecast', \"forecast_dist\", 'wp']\n",
    "def make_prediction_dataset(test, to_drop=to_drop_test):\n",
    "    test_to_predict = test.dropna(subset=['ws','u','v'], how = 'any') # keeps only lines with u,v,ws,wd\n",
    "    test_to_predict = test_to_predict[test_to_predict['wp'].isna()] # keeps only lines with no wp\n",
    "    test_to_predict = test_to_predict.sort_values(by=['date', 'forecast_time'], ascending = [True, False]).drop_duplicates(subset='date')\n",
    "    test_to_predict = test_to_predict.drop(to_drop, axis = 1)\n",
    "    scaler_X = MinMaxScaler(feature_range=(0,1))\n",
    "    test_to_predict = scaler_X.fit_transform(test_to_predict)\n",
    "    return test_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission_file(lst_X_trains, lst_y_trains, lst_tests, lst_models, dates,epochs):\n",
    "    i = 1\n",
    "    lst_prediction = []\n",
    "    lst_models_trained = []\n",
    "    for X, y, test, model in zip(lst_X_trains, lst_y_trains, lst_tests, lst_models):\n",
    "        print(f'--------------Model {i}--------------')\n",
    "        model.fit(X, y,epochs,verbose=0)\n",
    "        print(f'True:\\n\\tMin:{min(y)}\\n\\tMax:{max(y)}\\n\\tMean:{y.mean()}')\n",
    "        predictions = model.predict(test).reshape(len(test),)\n",
    "        print(f'Prediction:\\n\\tMin:{min(predictions)}\\n\\tMax:{max(predictions)}\\n\\tMean:{np.mean(predictions)}')\n",
    "        predictions = [min(y) if i < 0 else i for i in predictions]\n",
    "        predictions = [max(y) if i > max(y) else i for i in predictions]\n",
    "        print(f'Prediction corrected:\\n\\tMin:{min(predictions)}\\n\\tMax:{max(predictions)}\\n\\tMean:{np.mean(predictions)}')\n",
    "        lst_prediction.append(predictions)\n",
    "        lst_models_trained.append(model)\n",
    "        i+=1\n",
    "    \n",
    "    df_predictions = pd.DataFrame({\n",
    "        'date': test_dates,\n",
    "        'wp1': lst_prediction[0],\n",
    "        'wp2': lst_prediction[1],\n",
    "        'wp3': lst_prediction[2],\n",
    "        'wp4': lst_prediction[3],\n",
    "        'wp5': lst_prediction[4],\n",
    "        'wp6': lst_prediction[5],        \n",
    "    })\n",
    "    return df_predictions, lst_models_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x1=MinMaxScaler(feature_range=(0,1))\n",
    "scaler_x2=MinMaxScaler(feature_range=(0,1))\n",
    "scaler_x3=MinMaxScaler(feature_range=(0,1))\n",
    "scaler_x4=MinMaxScaler(feature_range=(0,1))\n",
    "scaler_x5=MinMaxScaler(feature_range=(0,1))\n",
    "scaler_x6=MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_scaled = scaler_x1.fit_transform(X1)\n",
    "X2_scaled = scaler_x2.fit_transform(X2)\n",
    "X3_scaled = scaler_x3.fit_transform(X3)\n",
    "X4_scaled = scaler_x4.fit_transform(X4)\n",
    "X5_scaled = scaler_x5.fit_transform(X5)\n",
    "X6_scaled = scaler_x6.fit_transform(X6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = build_model(X1,574)\n",
    "model_2 = build_model(X2,574)\n",
    "model_3 = build_model(X3,574)\n",
    "model_4 = build_model(X4,574)\n",
    "model_5 = build_model(X5,574)\n",
    "model_6 = build_model(X6,574)\n",
    "\n",
    "lst_models = [model_1, model_2, model_3, model_4, model_5, model_6]\n",
    "lst_X_trains = [X1_scaled, X2_scaled, X3_scaled, X4_scaled, X5_scaled, X6_scaled]\n",
    "lst_y_trains = [y1, y2, y3, y4, y5, y6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_tests = []\n",
    "for test in [test_wp1, test_wp2, test_wp3, test_wp4, test_wp5, test_wp6]:\n",
    "    test = make_prediction_dataset(test)\n",
    "    lst_tests.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions, lst_models_trained = make_submission_file(lst_X_trains, lst_y_trains, lst_tests, lst_models, test_dates,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.to_csv('Predictions/submission_nb_32_full_dnn.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_predictions[\"wp1\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_model = \"Models/DNN/DNN-wp1-3layers-574neurons.pkl\"\n",
    "with open(pkl_model, 'wb') as file:\n",
    "    pickle.dump(lst_models_trained[0], file)\n",
    "    \n",
    "    \n",
    "pkl_model = \"Models/DNN/DNN-wp2-3layers-574neurons.pkl\"\n",
    "with open(pkl_model, 'wb') as file:\n",
    "    pickle.dump(lst_models_trained[1], file)\n",
    "    \n",
    "\n",
    "pkl_model = \"Models/DNN/DNN-wp3-3layers-574neurons.pkl\"\n",
    "with open(pkl_model, 'wb') as file:\n",
    "    pickle.dump(lst_models_trained[2], file)\n",
    "\n",
    "\n",
    "pkl_model = \"Models/DNN/DNN-wp4-3layers-574neurons.pkl\"\n",
    "with open(pkl_model, 'wb') as file:\n",
    "    pickle.dump(lst_models_trained[3], file)\n",
    "\n",
    "\n",
    "pkl_model = \"Models/DNN/DNN-wp5-3layers-574neurons.pkl\"\n",
    "with open(pkl_model, 'wb') as file:\n",
    "    pickle.dump(lst_models_trained[4], file)\n",
    "\n",
    "\n",
    "pkl_model = \"Models/DNN/DNN-wp6-3layers-574neurons.pkl\"\n",
    "with open(pkl_model, 'wb') as file:\n",
    "    pickle.dump(lst_models_trained[5], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
