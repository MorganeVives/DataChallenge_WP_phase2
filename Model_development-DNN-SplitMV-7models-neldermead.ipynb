{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261ec557-50a0-4cd6-8bec-268d1dee685a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LGBM Model development - CV one model per fold - optim NelderMead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1399bb0f-0234-43a6-b724-73f21f9b28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout,LSTM\n",
    "from keras import regularizers, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a662ef3-a957-41fe-955d-1c5e99709226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna\n",
    "from vmdpy import VMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54f8406-f577-4449-8edb-631854c78d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc7dfff-4363-41ad-bfd2-5e5dd7c0ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.helper_functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c184629-3914-4a2c-ba3f-5f111909f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6333588b-f1b0-4a5b-afea-e53064be0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ccf0276-60ac-4599-a61f-328deed312d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x1f1517f92b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  ###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    "# Only allow a total of half the GPU memory to be allocatedC\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "# Create a session with the above options specified.\n",
    "tf.compat.v1.Session(config=config)\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617c0f5-e679-4a3f-9a62-12e03303d11f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d3258c-6931-447d-84b7-ac52ed05673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wp1 = pd.read_csv('Data/Preprocessing/WP1_train_preprocessed.csv', sep=',')\n",
    "train_wp2 = pd.read_csv('Data/Preprocessing/WP2_train_preprocessed.csv', sep=',')\n",
    "train_wp3 = pd.read_csv('Data/Preprocessing/WP3_train_preprocessed.csv', sep=',')\n",
    "train_wp4 = pd.read_csv('Data/Preprocessing/WP4_train_preprocessed.csv', sep=',')\n",
    "train_wp5 = pd.read_csv('Data/Preprocessing/WP5_train_preprocessed.csv', sep=',')\n",
    "train_wp6 = pd.read_csv('Data/Preprocessing/WP6_train_preprocessed.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f752d4-63ed-43b9-9315-41ecaa114ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wp1 = pd.read_csv('Data/Preprocessing/WP1_test_preprocessed.csv', sep=',')\n",
    "test_wp2 = pd.read_csv('Data/Preprocessing/WP2_test_preprocessed.csv', sep=',')\n",
    "test_wp3 = pd.read_csv('Data/Preprocessing/WP3_test_preprocessed.csv', sep=',')\n",
    "test_wp4 = pd.read_csv('Data/Preprocessing/WP4_test_preprocessed.csv', sep=',')\n",
    "test_wp5 = pd.read_csv('Data/Preprocessing/WP5_test_preprocessed.csv', sep=',')\n",
    "test_wp6 = pd.read_csv('Data/Preprocessing/WP6_test_preprocessed.csv', sep=',')\n",
    "test_dates = pd.read_csv('Data/Initial/test.csv', sep=',').date.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7540aa92-fc5e-4431-8e79-faba9f0cecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['date','wd','forecast_time', 'forecast', \"forecast_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0be88be-a88e-4777-a80f-66d64c4d0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_to_drop = [\n",
    "    'u_T_1', 'u_T_2', 'u_T_3', 'u_T_4', 'u_T_5', 'u_T_6', \n",
    "    'u_T_2_mean', 'u_T_3_mean', 'u_T_4_mean', 'u_T_5_mean', 'u_T_6_mean', 'u_T_7_mean',\n",
    "    'u_T_8_mean', 'u_T_9_mean', 'u_T_10_mean', 'u_T_11_mean', 'u_T_12_mean','u_T_24_mean',\n",
    "    'u_T_2_std', 'u_T_4_std', 'u_T_5_std', 'u_T_6_std',\n",
    "    'u_T_2_median', 'u_T_3_median', 'u_T_4_median', 'u_T_5_median', 'u_T_6_median', 'u_T_12_median','u_T_24_median', 'u_T_36_median',\n",
    "    'u_T_2_max', 'u_T_3_max', 'u_T_4_max', 'u_T_5_max', 'u_T_6_max', 'u_T_12_max',\n",
    "    'u_T_2_min', 'u_T_3_min', 'u_T_4_min', 'u_T_5_min', 'u_T_6_min', 'u_T_12_min',\n",
    "    'u2_T_1', 'u2_T_2', 'u2_T_3', 'u2_T_4', 'u2_T_5', 'u2_T_6', \n",
    "    'u2_T_2_mean', 'u2_T_3_mean', 'u2_T_4_mean', 'u2_T_5_mean', 'u2_T_6_mean', 'u2_T_7_mean',\n",
    "    'u2_T_8_mean', 'u2_T_9_mean', 'u2_T_10_mean', 'u2_T_11_mean', 'u2_T_12_mean','u2_T_24_mean',\n",
    "    'u2_T_2_std', 'u2_T_4_std', 'u2_T_5_std', 'u2_T_6_std', 'u2_T_24_std',\n",
    "    'u2_T_2_median', 'u2_T_3_median', 'u2_T_4_median', 'u2_T_5_median', 'u2_T_6_median', 'u2_T_12_median',\n",
    "    'u2_T_2_max','u2_T_3_max', 'u2_T_4_max','u2_T_5_max', 'u2_T_6_max', 'u2_T_12_max',\n",
    "    'u2_T_2_min', 'u2_T_3_min', 'u2_T_4_min', 'u2_T_5_min', 'u2_T_6_min',\n",
    "    'u2_T_12', 'u2_T_36_mean', 'u2_T_36_std', 'u2_T_24_median', 'u2_T_24_max',\n",
    "    'u_T_36_mean','u_T_12','u_T_24_max','u2_T_36_median','u_T_24_min'\n",
    "]\n",
    "ws_to_drop = [\n",
    "    'ws_T_1', 'ws_T_2', 'ws_T_3', 'ws_T_4', 'ws_T_5', 'ws_T_6', 'ws_T_7', 'ws_T_8', 'ws_T_10','ws_T_11', 'ws_T_12',\n",
    "    'ws_T_2_mean', 'ws_T_3_mean', 'ws_T_4_mean', 'ws_T_5_mean', 'ws_T_6_mean', 'ws_T_7_mean', 'ws_T_8_mean', 'ws_T_9_mean', \n",
    "    'ws_T_10_mean', 'ws_T_11_mean', 'ws_T_12_mean', 'ws_T_24_mean', \n",
    "    'ws_T_2_std', 'ws_T_3_std', 'ws_T_4_std', 'ws_T_5_std', \n",
    "    'ws_T_2_median', 'ws_T_3_median', 'ws_T_4_median', 'ws_T_5_median', 'ws_T_6_median',\n",
    "    'ws_T_12_median', 'ws_T_24_median', 'ws_T_36_median',\n",
    "    'ws_T_2_max', 'ws_T_3_max', 'ws_T_4_max', 'ws_T_5_max','ws_T_6_max', 'ws_T_12_max',\n",
    "     'ws_T_2_min', 'ws_T_3_min', 'ws_T_4_min', 'ws_T_5_min', 'ws_T_6_min', 'ws_T_12_min','ws_T_24_max','ws_T_24_min'\n",
    "]\n",
    "\n",
    "v_to_drop = [\n",
    "    'v_T_1', 'v_T_2', 'v_T_3', 'v_T_4', 'v_T_5', 'v_T_6', \n",
    "    'v_T_2_mean', 'v_T_3_mean', 'v_T_4_mean', 'v_T_5_mean', 'v_T_6_mean', 'v_T_7_mean',\n",
    "    'v_T_8_mean', 'v_T_9_mean', 'v_T_10_mean', 'v_T_11_mean', 'v_T_12_mean', 'v_T_24_mean','v_T_36_mean',\n",
    "    'v_T_3_std', 'v_T_4_std', 'v_T_5_std','v_T_6_std','v_T_24_std', 'v_T_36_median',\n",
    "    'v_T_2_median', 'v_T_3_median', 'v_T_4_median', 'v_T_5_median', 'v_T_6_median', \n",
    "    'v_T_2_max', 'v_T_3_max', 'v_T_4_max', 'v_T_5_max', 'v_T_6_max', 'v_T_12_max', \n",
    "    'v_T_2_min', 'v_T_3_min', 'v_T_4_min', 'v_T_5_min', 'v_T_6_min', 'v_T_12_min', \n",
    "    'v_T_36_min', 'v_T_36', 'v_T_24_max',  'v_T_12_median', 'v_T_24_median',\n",
    "]\n",
    "\n",
    "wd_to_drop = [\n",
    "    'coswd_1', 'coswd_2', 'coswd_3', 'coswd_4', 'coswd_5', 'coswd_6',\n",
    "    'coswd_2_mean', 'coswd_3_mean', 'coswd_4_mean', 'coswd_5_mean', 'coswd_6_mean', 'coswd_7_mean', \n",
    "    'coswd_8_mean', 'coswd_9_mean', 'coswd_10_mean', 'coswd_11_mean', 'coswd_12_mean', 'coswd_24_mean', \n",
    "    'coswd_3_std', 'coswd_4_std','coswd_5_std','coswd_2_median', 'coswd_3_median','coswd_4_median', \n",
    "    'coswd_5_median', 'coswd_6_median', 'coswd_36_median', 'coswd_24_median', 'coswd_12_median',\n",
    "    'coswd_2_max', 'coswd_3_max', 'coswd_4_max', 'coswd_5_max', 'coswd_6_max', 'coswd_12_max', 'coswd_24_max',\n",
    "    'coswd_2_min', 'coswd_3_min', 'coswd_4_min', 'coswd_5_min', 'coswd_6_min', 'coswd_12_min', 'coswd_24_min',\n",
    "    'ws_T_36_max', 'ws_T_36_min', 'coswd_12', 'coswd_24'\n",
    "]\n",
    "\n",
    "other_to_drop = [\n",
    "    'cos_day', 'u', 'v'\n",
    "]\n",
    "\n",
    "feature_corr = u_to_drop+ws_to_drop+v_to_drop+wd_to_drop+other_to_drop\n",
    "#to_drop = to_drop+feature_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0204367-344e-4837-9d27-c07833112862",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DNN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba314f6-b976-4482-9703-9f2bf7f575b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture du mod√®le:\n",
    "def build_model(data,n_neurons,nlayers,dropout,style,n):\n",
    "    layers=[]\n",
    "    if style=='linear':\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers)==0:\n",
    "                layers.append(Dense(n_neurons,activation='relu',input_shape=(data.shape[1],)))\n",
    "                layers.append(Dropout(dropout))\n",
    "            else:\n",
    "                layers.append(Dense(n_neurons,activation='relu'))\n",
    "                layers.append(Dropout(dropout))\n",
    "    else:\n",
    "        for i in range(nlayers):\n",
    "            if len(layers)==0:\n",
    "                layers.append(Dense(n_neurons,activation='relu',input_shape=(data.shape[1],)))\n",
    "                layers.append(Dropout(dropout))\n",
    "            elif (len(layers)+1)%nlayers==0:\n",
    "                layers.append(Dense(n_neurons,activation='relu'))\n",
    "                layers.append(Dropout(dropout))                \n",
    "            else:\n",
    "                layers.append(Dense(int(n_neurons*n),activation='relu'))\n",
    "                layers.append(Dropout(dropout))\n",
    "        \n",
    "    \n",
    "      \n",
    "    layers.append(Dense(1,activation='sigmoid'))\n",
    "\n",
    "    #with tf.device('/device:GPU:0'):\n",
    "    model = Sequential(layers)\n",
    "    model.compile(loss='mae',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6130af44-3f3b-44ac-9784-66af6eb81f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture du mod√®le:\n",
    "def build_model_v2(data,n_neurons,nlayers,dropout,style,n,norm):\n",
    "    input_tensor=Input(shape=(data.shape[1],))\n",
    "    if norm==1:\n",
    "        if style == 'cyclic':\n",
    "            for i in range(nlayers):\n",
    "                if i==0:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(input_tensor)\n",
    "                    x=layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                elif i%2!=0 and i!=0:\n",
    "                    x = layers.Dense(int(n_neurons*n),activation='relu')(x)\n",
    "                    x=layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                else:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(x)\n",
    "                    x=layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "        elif style == 'linear':\n",
    "            for i in range(nlayers):\n",
    "                if i==0:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(input_tensor)\n",
    "                    x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                else:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(x)\n",
    "                    x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "        elif style=='multiply':\n",
    "            for i in range(nlayers):\n",
    "                if i==0:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(input_tensor)\n",
    "                    x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                else:\n",
    "                    x = layers.Dense(int(n_neurons*n),activation='relu')(x)\n",
    "                    x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "        elif style=='power':\n",
    "            for i in range(nlayers):\n",
    "                if i==0:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(input_tensor)\n",
    "                    x=layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                else:\n",
    "                    x = layers.Dense(int(n_neurons*(n*i)),activation='relu')(x)\n",
    "                    x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "    else:\n",
    "        if style == 'cyclic':\n",
    "            for i in range(nlayers):\n",
    "                if i==0:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(input_tensor)\n",
    "                    #x=layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                elif i%2!=0 and i!=0:\n",
    "                    x = layers.Dense(int(n_neurons*n),activation='relu')(x)\n",
    "                    #x=layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                else:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(x)\n",
    "                    #x=layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "        elif style == 'linear':\n",
    "            for i in range(nlayers):\n",
    "                if i==0:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(input_tensor)\n",
    "                    #x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                else:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(x)\n",
    "                    #x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "        elif style=='multiply':\n",
    "            for i in range(nlayers):\n",
    "                if i==0:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(input_tensor)\n",
    "                    #x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                else:\n",
    "                    x = layers.Dense(int(n_neurons*n),activation='relu')(x)\n",
    "                    #x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "        elif style=='power':\n",
    "            for i in range(nlayers):\n",
    "                if i==0:\n",
    "                    x = layers.Dense(n_neurons,activation='relu')(input_tensor)\n",
    "                    #x=layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "                else:\n",
    "                    x = layers.Dense(int(n_neurons*(n*i)),activation='relu')(x)\n",
    "                    #x = layers.BatchNormalization()(x)\n",
    "                    x=layers.Dropout(dropout)(x)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    output_tensor=layers.Dense(1,activation='sigmoid')(x)\n",
    "    model = Model(input_tensor,output_tensor)\n",
    "    \n",
    "    model.compile(loss='mae',\n",
    "                optimizer='rmsprop',\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8b604e-c116-4dd2-8954-58ca0ec66ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7c70e7b-6243-448d-a253-fa4b23b3a621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hyperparametrization(trial, train_x, test_x, train_y, test_y):\n",
    "    epochs = trial.suggest_int('epochs',1,50)\n",
    "    n_neurons = trial.suggest_int('n_neurons',10,200)\n",
    "    nlayers = trial.suggest_int('nlayer',1,20)\n",
    "    dropout = trial.suggest_categorical('dropout',[0,0.1,0.25,0.5,0.3,0.4,0.2])\n",
    "    style = trial.suggest_categorical('style',['linear','cyclic','power','multiply'])\n",
    "    n = trial.suggest_int('n',1,4)\n",
    "    norm = trial.suggest_categorical('norm',[0,1])\n",
    "  \n",
    "      \n",
    "    tf_train = tf.data.Dataset.from_tensor_slices((train_x,train_y)).batch(36).prefetch(1)\n",
    "    tf_val = tf.data.Dataset.from_tensor_slices((test_x,test_y)).batch(48).prefetch(1)\n",
    "    print('#epochs:',epochs,'#layers:',nlayers,'#neurons',n_neurons)\n",
    "    model = build_model_v2(data=train_x,n_neurons=n_neurons,nlayers=nlayers,dropout=dropout,style=style,n=n,norm=norm)\n",
    "    model.fit(tf_train,verbose=0,epochs=epochs)   \n",
    "   \n",
    "    \n",
    "    return model.evaluate(tf_val,verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38829e20-4afe-4300-90c4-770c194f75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lst_model(cv,x,nlayers,dropout,n_neurons):\n",
    "    model_1=[]\n",
    "    model_2=[]\n",
    "    model_3=[]\n",
    "    model_4=[]\n",
    "    model_5=[]\n",
    "    model_6=[]\n",
    "    for i in range(cv):\n",
    "        model=build_model(data=x,\n",
    "                  n_neurons=n_neurons,\n",
    "                  nlayers=nlayers,\n",
    "                  dropout=dropout,\n",
    "                  style='other',\n",
    "                  n=2)\n",
    "        model_1+=[model]\n",
    "        model_2+=[model]\n",
    "        model_3+=[model]\n",
    "        model_4+=[model]\n",
    "        model_5+=[model]\n",
    "        model_6+=[model]\n",
    "    return [model_1,model_2,model_3,model_4,model_5,model_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b28b153-5e9a-4014-9873-e73acb5c3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lst_model_v2(cv,x,nlayers,dropout,n_neurons,style,n,norm):\n",
    "    model_1=[]\n",
    "    model_2=[]\n",
    "    model_3=[]\n",
    "    model_4=[]\n",
    "    model_5=[]\n",
    "    model_6=[]\n",
    "    for i in range(cv):\n",
    "        model=build_model_v2(data=x,\n",
    "                  n_neurons=n_neurons,\n",
    "                  nlayers=nlayers,\n",
    "                  dropout=dropout,\n",
    "                  style=style,\n",
    "                  n=n,\n",
    "                          norm=norm)\n",
    "        model_1+=[model]\n",
    "        model_2+=[model]\n",
    "        model_3+=[model]\n",
    "        model_4+=[model]\n",
    "        model_5+=[model]\n",
    "        model_6+=[model]\n",
    "    return [model_1,model_2,model_3,model_4,model_5,model_6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ff012-f8a3-49e7-a5e5-ab5afd7ae9a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d6619b7-3127-4695-9099-e2eb9b098eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wp1_X = train_wp1[[c for c in train_wp1 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X1 = wp1_X.drop('wp', axis=1)\n",
    "y1 = wp1_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963168d9-5603-43b1-849c-44027e745841",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp2_X = train_wp2[[c for c in train_wp2 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X2 = wp2_X.drop('wp', axis=1)\n",
    "y2 = wp2_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec857fb9-02d5-4524-8edd-43bbe5c79da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp3_X = train_wp3[[c for c in train_wp3 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X3 = wp3_X.drop('wp', axis = 1)\n",
    "y3 = wp3_X['wp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d5a0f0-2fc9-432c-bd7f-378fa37aeb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp4_X = train_wp4[[c for c in train_wp4 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X4 = wp4_X.drop('wp', axis = 1)\n",
    "y4 = wp4_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b517321-b217-46e1-b80a-3200bd61e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp5_X = train_wp5[[c for c in train_wp5 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X5 = wp5_X.drop('wp', axis = 1)\n",
    "y5 = wp5_X['wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "736f3edb-9f25-4a7b-a266-816425438b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wp6_X = train_wp6[[c for c in train_wp6 if c not in [\"wp\"]] + [\"wp\"]].drop(to_drop, axis = 1)\n",
    "X6 = wp6_X.drop('wp', axis = 1)\n",
    "y6 = wp6_X['wp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50001434-5b44-4a93-8005-ab7794310e1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating the 8 dataset per WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e786e516-a4fd-4101-9a59-e01adde2a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26f64486-fdcb-4668-bd4a-8bc534f90d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X1_train, lst_y1_train,lst_X1_test,lst_y1_test =  splitting_train_test_forecast(df_wp=train_wp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2828fed6-5ee9-4a76-9a1c-a4f2ed13fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lst_X1_train)):\n",
    "    lst_X1_train[i] = lst_X1_train[i].drop(to_drop,axis=1)\n",
    "    lst_X1_test[i] = lst_X1_test[i].drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0748e737-db22-40b2-baf0-de6ec557b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X2_train, lst_y2_train,lst_X2_test,lst_y2_test =  splitting_train_test_forecast(df_wp=train_wp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f506fd37-e04b-401d-979d-9a4def6b2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lst_X1_train)):\n",
    "    lst_X2_train[i] = lst_X2_train[i].drop(to_drop,axis=1)\n",
    "    lst_X2_test[i] = lst_X2_test[i].drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dbd8dbc-33ea-4728-9c9d-90dfe2e4044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X3_train, lst_y3_train,lst_X3_test,lst_y3_test =  splitting_train_test_forecast(df_wp=train_wp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "576eead0-95a8-4a4f-95d6-c8ffb74a2e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lst_X3_train)):\n",
    "    lst_X3_train[i] = lst_X3_train[i].drop(to_drop,axis=1)\n",
    "    lst_X3_test[i] = lst_X3_test[i].drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eec04690-f7d0-4730-b1fd-d528df1e82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X4_train, lst_y4_train,lst_X4_test,lst_y4_test =  splitting_train_test_forecast(df_wp=train_wp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9888c5d3-9cd2-4ecb-8248-044390910b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lst_X4_train)):\n",
    "    lst_X4_train[i] = lst_X4_train[i].drop(to_drop,axis=1)\n",
    "    lst_X4_test[i] = lst_X4_test[i].drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "475c5ffe-ade5-416a-8f33-57692438371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X5_train, lst_y5_train,lst_X5_test,lst_y5_test =  splitting_train_test_forecast(df_wp=train_wp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3669c3b6-fa82-4df8-9935-ba4e406826c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lst_X5_train)):\n",
    "    lst_X5_train[i] = lst_X5_train[i].drop(to_drop,axis=1)\n",
    "    lst_X5_test[i] = lst_X5_test[i].drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2f5b953-e947-4b48-9959-974294a2cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X6_train, lst_y6_train,lst_X6_test,lst_y6_test =  splitting_train_test_forecast(df_wp=train_wp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f85dc15b-cc9f-4628-b54b-5d4c0d18fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lst_X6_train)):\n",
    "    lst_X6_train[i] = lst_X6_train[i].drop(to_drop,axis=1)\n",
    "    lst_X6_test[i] = lst_X6_test[i].drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a3262d6-9405-487c-b270-64a5f1b0c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X = [X1, X2, X3, X4, X5, X6]\n",
    "lst_Y = [y1, y2, y3, y4, y5, y6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bca5e147-a542-4a27-8973-19cd426ebfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_X_trains_split = [lst_X1_train,lst_X2_train,lst_X3_train,lst_X4_train,lst_X5_train,lst_X6_train]\n",
    "lst_Y_trains_split = [lst_y1_train,lst_y2_train,lst_y3_train,lst_y4_train,lst_y5_train,lst_y6_train]\n",
    "lst_X_test_split = [lst_X1_test,lst_X2_test,lst_X3_test,lst_X4_test,lst_X5_test,lst_X6_test]\n",
    "lst_Y_test_split = [lst_y1_test,lst_y2_test,lst_y3_test,lst_y4_test,lst_y5_test,lst_y6_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23add09a-9419-46cf-925e-3e01aed49030",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9a275-46d3-4a2f-93d4-b6a6e0ab7102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efc011-c050-4e7f-b491-7169a2d56d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1=[]\n",
    "for x_train,y_train,x_test,y_test in zip(lst_X1_train,lst_y1_train,lst_X1_test,lst_y1_test):\n",
    "    def objective_wp1(trial):\n",
    "        \n",
    "        return hyperparametrization(trial, train_x=x_train, test_x=x_test, train_y=y_train, test_y=y_test)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective_wp1, n_trials=50)\n",
    "#write_results('Data/Hyperparametrization/lgbm_vmd_50trials_cv.xlsx', 'wp1', study.trials_dataframe())\n",
    "    best_trial = study.best_trial.params\n",
    "    print(best_trial)\n",
    "    params_1.append(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c4f4aa3-9564-41fe-87cc-011448397fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epochs': 45,\n",
       "  'n_neurons': 118,\n",
       "  'nlayer': 13,\n",
       "  'dropout': 0.1,\n",
       "  'style': 'cyclic',\n",
       "  'n': 2,\n",
       "  'norm': 0}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b70fc3-ed3d-4bba-8393-1619354c8bff",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65d70891-36d3-412f-8b6a-7f47e968a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------WP n¬∞ 1 --------\n",
      "--- set n¬∞ 0 -----\n",
      "-----fiting----\n",
      "--- 70.74976062774658 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.10503058135509491, 0.15564337372779846]\n",
      "--- 75.4876537322998 seconds ---\n",
      "--- set n¬∞ 1 -----\n",
      "-----fiting----\n",
      "--- 84.18728137016296 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.10395887494087219, 0.15179672837257385]\n",
      "--- 88.0834710597992 seconds ---\n",
      "--- set n¬∞ 2 -----\n",
      "-----fiting----\n",
      "--- 68.1334478855133 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.10423261672258377, 0.15388864278793335]\n",
      "--- 74.37038493156433 seconds ---\n",
      "--- set n¬∞ 3 -----\n",
      "-----fiting----\n",
      "--- 73.54607820510864 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.1062958836555481, 0.1557742804288864]\n",
      "--- 76.94760155677795 seconds ---\n",
      "--- set n¬∞ 4 -----\n",
      "-----fiting----\n",
      "--- 72.72862195968628 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11008834093809128, 0.15697148442268372]\n",
      "--- 76.34928512573242 seconds ---\n",
      "--- set n¬∞ 5 -----\n",
      "-----fiting----\n",
      "--- 60.177127838134766 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.10886234045028687, 0.15874633193016052]\n",
      "--- 70.54823875427246 seconds ---\n",
      "--- set n¬∞ 6 -----\n",
      "-----fiting----\n",
      "--- 60.67708349227905 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.1052173525094986, 0.15428927540779114]\n",
      "--- 64.1963837146759 seconds ---\n",
      "--------WP n¬∞ 2 --------\n",
      "--- set n¬∞ 0 -----\n",
      "-----fiting----\n",
      "--- 61.36973834037781 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11478065699338913, 0.17153094708919525]\n",
      "--- 64.64303636550903 seconds ---\n",
      "--- set n¬∞ 1 -----\n",
      "-----fiting----\n",
      "--- 63.829166650772095 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12410512566566467, 0.18053893744945526]\n",
      "--- 67.1692385673523 seconds ---\n",
      "--- set n¬∞ 2 -----\n",
      "-----fiting----\n",
      "--- 61.96730279922485 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11691401153802872, 0.1703355759382248]\n",
      "--- 65.32529878616333 seconds ---\n",
      "--- set n¬∞ 3 -----\n",
      "-----fiting----\n",
      "--- 60.700475454330444 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12131509184837341, 0.17472659051418304]\n",
      "--- 64.26117444038391 seconds ---\n",
      "--- set n¬∞ 4 -----\n",
      "-----fiting----\n",
      "--- 73.20619058609009 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11907146126031876, 0.17537075281143188]\n",
      "--- 77.67232394218445 seconds ---\n",
      "--- set n¬∞ 5 -----\n",
      "-----fiting----\n",
      "--- 59.8470504283905 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11932973563671112, 0.16953954100608826]\n",
      "--- 63.10970497131348 seconds ---\n",
      "--- set n¬∞ 6 -----\n",
      "-----fiting----\n",
      "--- 67.75254344940186 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11289912462234497, 0.16850696504116058]\n",
      "--- 71.32720685005188 seconds ---\n",
      "--------WP n¬∞ 3 --------\n",
      "--- set n¬∞ 0 -----\n",
      "-----fiting----\n",
      "--- 72.85396933555603 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11449476331472397, 0.17219313979148865]\n",
      "--- 76.73691964149475 seconds ---\n",
      "--- set n¬∞ 1 -----\n",
      "-----fiting----\n",
      "--- 59.889028787612915 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11825153231620789, 0.17303915321826935]\n",
      "--- 63.366676568984985 seconds ---\n",
      "--- set n¬∞ 2 -----\n",
      "-----fiting----\n",
      "--- 69.10092234611511 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11655554920434952, 0.17309264838695526]\n",
      "--- 72.47194862365723 seconds ---\n",
      "--- set n¬∞ 3 -----\n",
      "-----fiting----\n",
      "--- 63.37401270866394 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11977636814117432, 0.17794492840766907]\n",
      "--- 66.75418043136597 seconds ---\n",
      "--- set n¬∞ 4 -----\n",
      "-----fiting----\n",
      "--- 62.998003005981445 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11685997247695923, 0.17411686480045319]\n",
      "--- 66.91902303695679 seconds ---\n",
      "--- set n¬∞ 5 -----\n",
      "-----fiting----\n",
      "--- 65.28739929199219 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11469538509845734, 0.1705789417028427]\n",
      "--- 68.91886138916016 seconds ---\n",
      "--- set n¬∞ 6 -----\n",
      "-----fiting----\n",
      "--- 70.88659834861755 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11640819162130356, 0.17440932989120483]\n",
      "--- 74.65818047523499 seconds ---\n",
      "--------WP n¬∞ 4 --------\n",
      "--- set n¬∞ 0 -----\n",
      "-----fiting----\n",
      "--- 71.74753546714783 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11982200294733047, 0.172524556517601]\n",
      "--- 75.17967557907104 seconds ---\n",
      "--- set n¬∞ 1 -----\n",
      "-----fiting----\n",
      "--- 71.34996318817139 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12512139976024628, 0.17801305651664734]\n",
      "--- 76.15911078453064 seconds ---\n",
      "--- set n¬∞ 2 -----\n",
      "-----fiting----\n",
      "--- 68.31595659255981 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.10891903936862946, 0.15694916248321533]\n",
      "--- 72.05592370033264 seconds ---\n",
      "--- set n¬∞ 3 -----\n",
      "-----fiting----\n",
      "--- 70.39491176605225 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11704981327056885, 0.16823478043079376]\n",
      "--- 75.01014876365662 seconds ---\n",
      "--- set n¬∞ 4 -----\n",
      "-----fiting----\n",
      "--- 75.43436861038208 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11743269860744476, 0.1692081093788147]\n",
      "--- 79.66275763511658 seconds ---\n",
      "--- set n¬∞ 5 -----\n",
      "-----fiting----\n",
      "--- 70.30323505401611 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11304302513599396, 0.1615997552871704]\n",
      "--- 74.23639416694641 seconds ---\n",
      "--- set n¬∞ 6 -----\n",
      "-----fiting----\n",
      "--- 78.62449026107788 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.11586425453424454, 0.16784776747226715]\n",
      "--- 82.05157136917114 seconds ---\n",
      "--------WP n¬∞ 5 --------\n",
      "--- set n¬∞ 0 -----\n",
      "-----fiting----\n",
      "--- 59.17325186729431 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.1210508644580841, 0.17526206374168396]\n",
      "--- 62.61350417137146 seconds ---\n",
      "--- set n¬∞ 1 -----\n",
      "-----fiting----\n",
      "--- 63.555567502975464 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12030261009931564, 0.17290422320365906]\n",
      "--- 66.95048069953918 seconds ---\n",
      "--- set n¬∞ 2 -----\n",
      "-----fiting----\n",
      "--- 59.14467430114746 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.13575518131256104, 0.19813893735408783]\n",
      "--- 62.47072172164917 seconds ---\n",
      "--- set n¬∞ 3 -----\n",
      "-----fiting----\n",
      "--- 56.65008807182312 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12814107537269592, 0.18477173149585724]\n",
      "--- 59.93353247642517 seconds ---\n",
      "--- set n¬∞ 4 -----\n",
      "-----fiting----\n",
      "--- 68.50243353843689 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12779460847377777, 0.18271291255950928]\n",
      "--- 72.42767810821533 seconds ---\n",
      "--- set n¬∞ 5 -----\n",
      "-----fiting----\n",
      "--- 64.28896856307983 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12649495899677277, 0.18052521347999573]\n",
      "--- 68.27644157409668 seconds ---\n",
      "--- set n¬∞ 6 -----\n",
      "-----fiting----\n",
      "--- 64.97103023529053 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12263050675392151, 0.17814061045646667]\n",
      "--- 68.25783085823059 seconds ---\n",
      "--------WP n¬∞ 6 --------\n",
      "--- set n¬∞ 0 -----\n",
      "-----fiting----\n",
      "--- 61.524738788604736 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.13858380913734436, 0.1972675323486328]\n",
      "--- 64.80121779441833 seconds ---\n",
      "--- set n¬∞ 1 -----\n",
      "-----fiting----\n",
      "--- 63.90932846069336 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12969979643821716, 0.18401387333869934]\n",
      "--- 67.22181868553162 seconds ---\n",
      "--- set n¬∞ 2 -----\n",
      "-----fiting----\n",
      "--- 55.535014629364014 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12747164070606232, 0.18218351900577545]\n",
      "--- 58.8236448764801 seconds ---\n",
      "--- set n¬∞ 3 -----\n",
      "-----fiting----\n",
      "--- 55.94059872627258 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.13143885135650635, 0.18594679236412048]\n",
      "--- 59.20872640609741 seconds ---\n",
      "--- set n¬∞ 4 -----\n",
      "-----fiting----\n",
      "--- 56.73828959465027 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.13678434491157532, 0.1941385418176651]\n",
      "--- 59.8959379196167 seconds ---\n",
      "--- set n¬∞ 5 -----\n",
      "-----fiting----\n",
      "--- 55.96318197250366 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.12862563133239746, 0.18367263674736023]\n",
      "--- 59.24686121940613 seconds ---\n",
      "--- set n¬∞ 6 -----\n",
      "-----fiting----\n",
      "--- 61.26230192184448 seconds ---\n",
      "Evaluate on test data\n",
      "test loss, test acc: [0.13161887228488922, 0.1880706548690796]\n",
      "--- 64.40829992294312 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "#with tf.device('/device:GPU:0'):\n",
    "lst_model = create_lst_model_v2(cv=7,x=X1,nlayers=5,dropout=0.1,n_neurons=64,style='linear',n=2,norm=0)\n",
    "#print(lst_model[0][0].summary())\n",
    "#print('---check eights on first layer-----')\n",
    "#print(lst_model[0][0].get_weights()[0])\n",
    "lst_predictions_all=[]\n",
    "t=1\n",
    "for model,x_all,y_all,x,y,x_test,y_test in zip(lst_model, lst_X,lst_Y, lst_X_trains_split, lst_Y_trains_split, lst_X_test_split, lst_Y_test_split):\n",
    "    lst_predictions=[]\n",
    "    print('--------WP n¬∞',t,'--------')\n",
    "    #scaler_x = MinMaxScaler(feature_range=(0,1))\n",
    "    for i in range(7):\n",
    "        print('--- set n¬∞',i,'-----')\n",
    "        clf=model[i]\n",
    "        #x_train_scaled = scaler_x.fit_transform(x[i])\n",
    "        #x_test_scaled=scaler_x.transform(x_test[i])\n",
    "        #x_all_scaled=scaler_x.transform(x_all)\n",
    "        #scaler_name = 'scaler_{0}_{1}'.format(t,i)\n",
    "        tf_train = tf.data.Dataset.from_tensor_slices((x[i],y[i])).batch(36).prefetch(1)\n",
    "        tf_val = tf.data.Dataset.from_tensor_slices((x_test[i],y_test[i])).batch(48).prefetch(1)\n",
    "        tf_all = tf.data.Dataset.from_tensor_slices((x_all,y_all)).batch(84).prefetch(1)\n",
    "        print('-----fiting----')\n",
    "        start_time = time.time()\n",
    "        clf.fit(tf_train,  verbose=0, epochs=10) #,\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"Evaluate on test data\")\n",
    "        results = clf.evaluate(tf_val,verbose=0) # \n",
    "        \n",
    "        print(\"test loss, test acc:\", results)\n",
    "        lst_predictions.append(clf.predict(tf_all))\n",
    "        #pickle.dump(scaler_x,open(scaler_name,\"wb\"))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    lst_predictions_all.append(lst_predictions)\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08911be8-0fae-4632-a8e8-8e107976d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_all=[]\n",
    "for pred,y in zip(lst_predictions_all,lst_Y):\n",
    "    weights=[]\n",
    "    for i in range(7):\n",
    "        weights.append(0)\n",
    "        \n",
    "    def mae_func(weights):\n",
    "        #final_prediction=0\n",
    "        for i in range(len(weights)):\n",
    "            if i==0:\n",
    "                final_prediction = weights[i]*pred[i]\n",
    "            else:\n",
    "                final_prediction += weights[i]*pred[i]\n",
    "        return mean_absolute_error(y, final_prediction)\n",
    "    res = minimize(mae_func, weights, method='Nelder-Mead')\n",
    "    weights_all.append(res['x'])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76bd00a3-0f21-45e8-8e49-6d8e38950d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(weights_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddea936d-2fde-484e-bec0-ba50e358d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_test = ['date','wd','forecast_time', 'forecast', \"forecast_dist\", 'wp']#+feature_corr\n",
    "def make_prediction_dataset(test, to_drop=to_drop_test):\n",
    "    test_to_predict = test.dropna(subset=['ws','u','v'], how = 'any') # keeps only lines with u,v,ws,wd\n",
    "    test_to_predict = test_to_predict[test_to_predict['wp'].isna()] # keeps only lines with no wp\n",
    "    test_to_predict = test_to_predict.sort_values(by=['date', 'forecast_time'], ascending = [True, False]).drop_duplicates(subset='date')\n",
    "    test_to_predict = test_to_predict.drop(to_drop, axis = 1)\n",
    "    return test_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd2987dc-69fb-4490-b24c-4fab7ebb6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_tests = []\n",
    "for test in [test_wp1, test_wp2, test_wp3, test_wp4, test_wp5, test_wp6]:\n",
    "    test = make_prediction_dataset(test)\n",
    "    lst_tests.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd3a8490-22a9-415f-8006-40c817bbe994",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_final_prediction=[]\n",
    "for weights,model,test,x_train,y_train in zip(weights_all,lst_model,lst_tests,lst_X_trains_split,lst_Y_trains_split):\n",
    "    t=1\n",
    "    for i in range(7):\n",
    "        clf=model[i]\n",
    "        #scaler = pickle.load(open('scaler_{0}_{1}'.format(t,i),\"rb\"))\n",
    "        #x_test=scaler.transform(test)\n",
    "        #clf.fit(x_train[i],y_train[i],verbose=0,batch_size=36)\n",
    "        x_test = tf.data.Dataset.from_tensor_slices((test)).batch(84).prefetch(1)\n",
    "        if i==0:\n",
    "            y_pred=clf.predict(x_test,batch_size=84)*weights[i]\n",
    "        else:\n",
    "            y_pred+=clf.predict(x_test,batch_size=84)*weights[i]\n",
    "    lst_final_prediction.append(y_pred)\n",
    "    t+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc2f5bb3-482b-4685-9edc-455e7769383c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7440,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(lst_final_prediction[0].reshape(7440,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "851f2c25-1fb3-4901-a478-b041c7361c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame({\n",
    "        'date': test_dates,\n",
    "        'wp1': lst_final_prediction[0].reshape(7440,),\n",
    "        'wp2': lst_final_prediction[1].reshape(7440,),\n",
    "        'wp3': lst_final_prediction[2].reshape(7440,),\n",
    "        'wp4': lst_final_prediction[3].reshape(7440,),\n",
    "        'wp5': lst_final_prediction[4].reshape(7440,),\n",
    "        'wp6': lst_final_prediction[5].reshape(7440,),        \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b82ee477-0e9a-4398-b111-50bfb57c441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sub = 50\n",
    "model = \"dnn_7models_traintestsplit\"\n",
    "prepro = 'RobustScaler'\n",
    "postpro = \"Prediction limited by 0-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9327f0f7-ac06-4692-997d-84c80e68c82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wp1</th>\n",
       "      <th>wp2</th>\n",
       "      <th>wp3</th>\n",
       "      <th>wp4</th>\n",
       "      <th>wp5</th>\n",
       "      <th>wp6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011010101</td>\n",
       "      <td>0.529483</td>\n",
       "      <td>0.270229</td>\n",
       "      <td>1.425836e-07</td>\n",
       "      <td>0.340227</td>\n",
       "      <td>0.446056</td>\n",
       "      <td>0.437530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011010102</td>\n",
       "      <td>0.520036</td>\n",
       "      <td>0.312680</td>\n",
       "      <td>-1.659469e-07</td>\n",
       "      <td>0.341136</td>\n",
       "      <td>0.436530</td>\n",
       "      <td>0.417496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011010103</td>\n",
       "      <td>0.524673</td>\n",
       "      <td>0.330738</td>\n",
       "      <td>7.823510e-05</td>\n",
       "      <td>0.340288</td>\n",
       "      <td>0.464695</td>\n",
       "      <td>0.426882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011010104</td>\n",
       "      <td>0.541555</td>\n",
       "      <td>0.327666</td>\n",
       "      <td>3.110182e-02</td>\n",
       "      <td>0.372731</td>\n",
       "      <td>0.505008</td>\n",
       "      <td>0.428434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011010105</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.308715</td>\n",
       "      <td>1.899543e-01</td>\n",
       "      <td>0.407729</td>\n",
       "      <td>0.550857</td>\n",
       "      <td>0.425569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       wp1       wp2           wp3       wp4       wp5       wp6\n",
       "0  2011010101  0.529483  0.270229  1.425836e-07  0.340227  0.446056  0.437530\n",
       "1  2011010102  0.520036  0.312680 -1.659469e-07  0.341136  0.436530  0.417496\n",
       "2  2011010103  0.524673  0.330738  7.823510e-05  0.340288  0.464695  0.426882\n",
       "3  2011010104  0.541555  0.327666  3.110182e-02  0.372731  0.505008  0.428434\n",
       "4  2011010105  0.564103  0.308715  1.899543e-01  0.407729  0.550857  0.425569"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2da8a01e-0303-45f0-b6c4-d67d55e654f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predictions.to_csv('Predictions/submission_nb_10_full_maxabs-lgbm-featselect.csv', index=False, sep=';')\n",
    "df_predictions.to_csv(f'Predictions/submission_nb_{nb_sub}_{model}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae81d3-9c79-488b-ab82-b2cb7dc5a1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
